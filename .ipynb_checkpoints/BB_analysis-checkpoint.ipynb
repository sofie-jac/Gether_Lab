{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c77fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "#from ipynb.fs.full.BB_functions import files_from_folder, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0eab208",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(r'C:\\Users\\UCPH\\gabi\\thesis analysis\\test1')\n",
    "input_path2 = Path(r'C:\\Users\\UCPH\\gabi\\thesis analysis\\test2')\n",
    "input_path3 = Path(r'C:\\Users\\UCPH\\gabi\\thesis analysis\\test3')\n",
    "\n",
    "save_path = Path(r'C:\\Users\\UCPH\\gabi\\thesis analysis')\n",
    "\n",
    "combined_path = os.path.join(input_path, input_path2, input_path3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6d7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = files_from_folder(combined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f98cf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>1</th>\n",
       "      <th>Nose_x</th>\n",
       "      <th>FrontLimb_x</th>\n",
       "      <th>HindLimb_x</th>\n",
       "      <th>TailBase_x</th>\n",
       "      <th>Start_x</th>\n",
       "      <th>End_x</th>\n",
       "      <th>Nose_y</th>\n",
       "      <th>FrontLimb_y</th>\n",
       "      <th>HindLimb_y</th>\n",
       "      <th>TailBase_y</th>\n",
       "      <th>Start_y</th>\n",
       "      <th>End_y</th>\n",
       "      <th>Nose_likelihood</th>\n",
       "      <th>FrontLimb_likelihood</th>\n",
       "      <th>HindLimb_likelihood</th>\n",
       "      <th>TailBase_likelihood</th>\n",
       "      <th>Start_likelihood</th>\n",
       "      <th>End_likelihood</th>\n",
       "      <th>Name File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118.705391</td>\n",
       "      <td>84.356087</td>\n",
       "      <td>45.021431</td>\n",
       "      <td>19.125366</td>\n",
       "      <td>23.093014</td>\n",
       "      <td>1123.715210</td>\n",
       "      <td>228.196945</td>\n",
       "      <td>230.244873</td>\n",
       "      <td>226.887482</td>\n",
       "      <td>183.649704</td>\n",
       "      <td>227.580566</td>\n",
       "      <td>215.952133</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.997784</td>\n",
       "      <td>0.998932</td>\n",
       "      <td>0.998948</td>\n",
       "      <td>0.892152</td>\n",
       "      <td>0.999968</td>\n",
       "      <td>1_POST_BB_15_H1,6_M_KI_T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118.491753</td>\n",
       "      <td>83.447426</td>\n",
       "      <td>44.274624</td>\n",
       "      <td>15.888601</td>\n",
       "      <td>22.109818</td>\n",
       "      <td>1124.019043</td>\n",
       "      <td>229.310165</td>\n",
       "      <td>231.698654</td>\n",
       "      <td>231.012070</td>\n",
       "      <td>192.338211</td>\n",
       "      <td>230.860199</td>\n",
       "      <td>216.730377</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.998824</td>\n",
       "      <td>0.999738</td>\n",
       "      <td>0.999546</td>\n",
       "      <td>0.803088</td>\n",
       "      <td>0.999929</td>\n",
       "      <td>1_POST_BB_15_H1,6_M_KI_T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>118.520317</td>\n",
       "      <td>84.808968</td>\n",
       "      <td>44.813637</td>\n",
       "      <td>12.805939</td>\n",
       "      <td>20.239227</td>\n",
       "      <td>1125.334106</td>\n",
       "      <td>228.522736</td>\n",
       "      <td>232.123550</td>\n",
       "      <td>234.702774</td>\n",
       "      <td>200.734406</td>\n",
       "      <td>235.099564</td>\n",
       "      <td>216.744690</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999043</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>0.999714</td>\n",
       "      <td>0.763183</td>\n",
       "      <td>0.999882</td>\n",
       "      <td>1_POST_BB_15_H1,6_M_KI_T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>118.480530</td>\n",
       "      <td>84.795830</td>\n",
       "      <td>44.797939</td>\n",
       "      <td>12.839941</td>\n",
       "      <td>20.288731</td>\n",
       "      <td>1124.635132</td>\n",
       "      <td>228.549042</td>\n",
       "      <td>232.210297</td>\n",
       "      <td>234.747940</td>\n",
       "      <td>200.774094</td>\n",
       "      <td>234.514145</td>\n",
       "      <td>217.311172</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.998965</td>\n",
       "      <td>0.999528</td>\n",
       "      <td>0.999717</td>\n",
       "      <td>0.748803</td>\n",
       "      <td>0.999735</td>\n",
       "      <td>1_POST_BB_15_H1,6_M_KI_T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>119.976601</td>\n",
       "      <td>86.535606</td>\n",
       "      <td>46.187389</td>\n",
       "      <td>12.991815</td>\n",
       "      <td>22.256866</td>\n",
       "      <td>1123.147583</td>\n",
       "      <td>228.673187</td>\n",
       "      <td>232.188736</td>\n",
       "      <td>236.544952</td>\n",
       "      <td>212.412521</td>\n",
       "      <td>232.169922</td>\n",
       "      <td>218.792526</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999519</td>\n",
       "      <td>0.998655</td>\n",
       "      <td>0.999944</td>\n",
       "      <td>0.542716</td>\n",
       "      <td>0.999966</td>\n",
       "      <td>1_POST_BB_15_H1,6_M_KI_T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1205.615845</td>\n",
       "      <td>1169.924805</td>\n",
       "      <td>1145.297119</td>\n",
       "      <td>1111.620605</td>\n",
       "      <td>23.387436</td>\n",
       "      <td>1107.312622</td>\n",
       "      <td>228.472900</td>\n",
       "      <td>238.173798</td>\n",
       "      <td>244.961700</td>\n",
       "      <td>233.857208</td>\n",
       "      <td>236.857986</td>\n",
       "      <td>234.414017</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999933</td>\n",
       "      <td>0.996813</td>\n",
       "      <td>0.997857</td>\n",
       "      <td>0.999872</td>\n",
       "      <td>0.083438</td>\n",
       "      <td>1_PRE_BB_37_VEHE_M_WT_T4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1205.666870</td>\n",
       "      <td>1169.890503</td>\n",
       "      <td>1145.133057</td>\n",
       "      <td>1111.611084</td>\n",
       "      <td>23.399660</td>\n",
       "      <td>1107.379395</td>\n",
       "      <td>228.453888</td>\n",
       "      <td>238.139267</td>\n",
       "      <td>244.916611</td>\n",
       "      <td>234.013306</td>\n",
       "      <td>236.831055</td>\n",
       "      <td>234.467422</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999936</td>\n",
       "      <td>0.996630</td>\n",
       "      <td>0.997701</td>\n",
       "      <td>0.999871</td>\n",
       "      <td>0.095527</td>\n",
       "      <td>1_PRE_BB_37_VEHE_M_WT_T4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1210.462280</td>\n",
       "      <td>1171.872681</td>\n",
       "      <td>1152.651123</td>\n",
       "      <td>1114.580933</td>\n",
       "      <td>23.696810</td>\n",
       "      <td>1108.752563</td>\n",
       "      <td>219.712067</td>\n",
       "      <td>236.656372</td>\n",
       "      <td>241.236664</td>\n",
       "      <td>234.656494</td>\n",
       "      <td>235.570328</td>\n",
       "      <td>232.313995</td>\n",
       "      <td>0.999956</td>\n",
       "      <td>0.999749</td>\n",
       "      <td>0.979470</td>\n",
       "      <td>0.999672</td>\n",
       "      <td>0.999868</td>\n",
       "      <td>0.041478</td>\n",
       "      <td>1_PRE_BB_37_VEHE_M_WT_T4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1206.215454</td>\n",
       "      <td>1182.290039</td>\n",
       "      <td>1157.067505</td>\n",
       "      <td>1115.402100</td>\n",
       "      <td>23.347319</td>\n",
       "      <td>1108.674561</td>\n",
       "      <td>220.582397</td>\n",
       "      <td>235.807373</td>\n",
       "      <td>243.363998</td>\n",
       "      <td>237.354004</td>\n",
       "      <td>235.940796</td>\n",
       "      <td>239.704620</td>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>0.991576</td>\n",
       "      <td>0.999746</td>\n",
       "      <td>0.999876</td>\n",
       "      <td>0.129351</td>\n",
       "      <td>1_PRE_BB_37_VEHE_M_WT_T4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1206.261353</td>\n",
       "      <td>1182.320312</td>\n",
       "      <td>1157.090942</td>\n",
       "      <td>1115.445923</td>\n",
       "      <td>25.118343</td>\n",
       "      <td>1108.434082</td>\n",
       "      <td>220.584793</td>\n",
       "      <td>235.659286</td>\n",
       "      <td>243.248062</td>\n",
       "      <td>237.258453</td>\n",
       "      <td>227.955475</td>\n",
       "      <td>239.487778</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>0.999910</td>\n",
       "      <td>0.992120</td>\n",
       "      <td>0.999745</td>\n",
       "      <td>0.999650</td>\n",
       "      <td>0.159903</td>\n",
       "      <td>1_PRE_BB_37_VEHE_M_WT_T4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97862 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "1        Nose_x  FrontLimb_x   HindLimb_x   TailBase_x    Start_x  \\\n",
       "3    118.705391    84.356087    45.021431    19.125366  23.093014   \n",
       "4    118.491753    83.447426    44.274624    15.888601  22.109818   \n",
       "5    118.520317    84.808968    44.813637    12.805939  20.239227   \n",
       "6    118.480530    84.795830    44.797939    12.839941  20.288731   \n",
       "7    119.976601    86.535606    46.187389    12.991815  22.256866   \n",
       "..          ...          ...          ...          ...        ...   \n",
       "83  1205.615845  1169.924805  1145.297119  1111.620605  23.387436   \n",
       "84  1205.666870  1169.890503  1145.133057  1111.611084  23.399660   \n",
       "85  1210.462280  1171.872681  1152.651123  1114.580933  23.696810   \n",
       "86  1206.215454  1182.290039  1157.067505  1115.402100  23.347319   \n",
       "87  1206.261353  1182.320312  1157.090942  1115.445923  25.118343   \n",
       "\n",
       "1         End_x      Nose_y  FrontLimb_y  HindLimb_y  TailBase_y     Start_y  \\\n",
       "3   1123.715210  228.196945   230.244873  226.887482  183.649704  227.580566   \n",
       "4   1124.019043  229.310165   231.698654  231.012070  192.338211  230.860199   \n",
       "5   1125.334106  228.522736   232.123550  234.702774  200.734406  235.099564   \n",
       "6   1124.635132  228.549042   232.210297  234.747940  200.774094  234.514145   \n",
       "7   1123.147583  228.673187   232.188736  236.544952  212.412521  232.169922   \n",
       "..          ...         ...          ...         ...         ...         ...   \n",
       "83  1107.312622  228.472900   238.173798  244.961700  233.857208  236.857986   \n",
       "84  1107.379395  228.453888   238.139267  244.916611  234.013306  236.831055   \n",
       "85  1108.752563  219.712067   236.656372  241.236664  234.656494  235.570328   \n",
       "86  1108.674561  220.582397   235.807373  243.363998  237.354004  235.940796   \n",
       "87  1108.434082  220.584793   235.659286  243.248062  237.258453  227.955475   \n",
       "\n",
       "1        End_y  Nose_likelihood  FrontLimb_likelihood  HindLimb_likelihood  \\\n",
       "3   215.952133         0.999998              0.997784             0.998932   \n",
       "4   216.730377         0.999997              0.998824             0.999738   \n",
       "5   216.744690         0.999997              0.999043             0.999532   \n",
       "6   217.311172         0.999997              0.998965             0.999528   \n",
       "7   218.792526         0.999997              0.999519             0.998655   \n",
       "..         ...              ...                   ...                  ...   \n",
       "83  234.414017         0.999999              0.999933             0.996813   \n",
       "84  234.467422         0.999999              0.999936             0.996630   \n",
       "85  232.313995         0.999956              0.999749             0.979470   \n",
       "86  239.704620         0.999987              0.999916             0.991576   \n",
       "87  239.487778         0.999988              0.999910             0.992120   \n",
       "\n",
       "1   TailBase_likelihood  Start_likelihood  End_likelihood  \\\n",
       "3              0.998948          0.892152        0.999968   \n",
       "4              0.999546          0.803088        0.999929   \n",
       "5              0.999714          0.763183        0.999882   \n",
       "6              0.999717          0.748803        0.999735   \n",
       "7              0.999944          0.542716        0.999966   \n",
       "..                  ...               ...             ...   \n",
       "83             0.997857          0.999872        0.083438   \n",
       "84             0.997701          0.999871        0.095527   \n",
       "85             0.999672          0.999868        0.041478   \n",
       "86             0.999746          0.999876        0.129351   \n",
       "87             0.999745          0.999650        0.159903   \n",
       "\n",
       "1                   Name File  \n",
       "3   1_POST_BB_15_H1,6_M_KI_T1  \n",
       "4   1_POST_BB_15_H1,6_M_KI_T1  \n",
       "5   1_POST_BB_15_H1,6_M_KI_T1  \n",
       "6   1_POST_BB_15_H1,6_M_KI_T1  \n",
       "7   1_POST_BB_15_H1,6_M_KI_T1  \n",
       "..                        ...  \n",
       "83   1_PRE_BB_37_VEHE_M_WT_T4  \n",
       "84   1_PRE_BB_37_VEHE_M_WT_T4  \n",
       "85   1_PRE_BB_37_VEHE_M_WT_T4  \n",
       "86   1_PRE_BB_37_VEHE_M_WT_T4  \n",
       "87   1_PRE_BB_37_VEHE_M_WT_T4  \n",
       "\n",
       "[97862 rows x 19 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'Name File' column into multiple columns\n",
    "df[['Cohort', 'Pre_Post', 'Test_Type', 'Animal_ID', 'Drug_Dose', 'Genotype', 'Weight', 'Gender', 'Trial']] = df['Name File'].str.split('_', expand=True)\n",
    "\n",
    "# Drop the original 'Name File' column if needed\n",
    "# df.drop('Name File', axis=1, inplace=True)\n",
    "\n",
    "# Split 'drug_dose' into 'Drug' and 'Dose' columns\n",
    "#df[['Drug', 'Dose']] = df['Drug_Dose'].str.extract(r'([A-Za-z]+)(\\d+,\\d+)')\n",
    "\n",
    "# Extract only numbers from the 'Trial' column\n",
    "df['Trial'] = df['Trial'].str.extract('(\\d+)')\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12352d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through columns and set values to NaN based on condition\n",
    "for col in df.columns:\n",
    "    if col.endswith('_likelihood'):\n",
    "        mask = df[col] < 0.7\n",
    "        prefix = col[:-11]  # Remove '_likelihood' to get the prefix\n",
    "        for suffix in ['x', 'y']:\n",
    "            column_to_set_nan = f'{prefix}_{suffix}'\n",
    "            df.loc[mask, column_to_set_nan] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5811945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the adjusted mean\n",
    "def calculate_adjusted_mean(group):\n",
    "    filtered_start_x = group[group['Start_likelihood'] > 0.95]['Start_x']\n",
    "    filtered_start_y = group[group['Start_likelihood'] > 0.95]['Start_y']\n",
    "    filtered_end_x = group[group['End_likelihood'] > 0.95]['End_x']\n",
    "    filtered_end_y = group[group['End_likelihood'] > 0.95]['End_y']\n",
    "    \n",
    "    start_x_adjusted_mean = filtered_start_x.mean()\n",
    "    start_y_adjusted_mean = filtered_start_y.mean()\n",
    "    end_x_adjusted_mean = filtered_end_x.mean()\n",
    "    end_y_adjusted_mean = filtered_end_y.mean()\n",
    "    \n",
    "    group['Start_x_adjust'] = start_x_adjusted_mean\n",
    "    group['Start_y_adjust'] = start_y_adjusted_mean\n",
    "    group['End_x_adjust'] = end_x_adjusted_mean\n",
    "    group['End_y_adjust'] = end_y_adjusted_mean\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Group by 'Name File' and apply the calculation\n",
    "df = df.groupby('Name File').apply(calculate_adjusted_mean)\n",
    "\n",
    "# Fill NaN values in the new columns with appropriate values\n",
    "df['Start_x_adjust'].fillna(df['Start_x'], inplace=True)\n",
    "df['Start_y_adjust'].fillna(df['Start_y'], inplace=True)\n",
    "df['End_x_adjust'].fillna(df['End_x'], inplace=True)\n",
    "df['End_y_adjust'].fillna(df['End_y'], inplace=True)\n",
    "\n",
    "# Drop the original 'Start_likelihood' and 'End_likelihood' columns if needed\n",
    "# df.drop(['Start_likelihood', 'End_likelihood'], axis=1, inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine Framerate and y_pixels based on Cohort\n",
    "def set_framerate_and_ypixels(row):\n",
    "    if row['Cohort'] == 1:\n",
    "        return 15, 720\n",
    "    elif row['Cohort'] == 2:\n",
    "        return 60, 1080\n",
    "    else:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "# Convert 'Cohort' to integer\n",
    "df['Cohort'] = df['Cohort'].astype(int)\n",
    "\n",
    "# Apply the function to create the 'Framerate' and 'y_pixels' columns\n",
    "df[['Framerate', 'y_pixels']] = df.apply(set_framerate_and_ypixels, axis=1, result_type='expand')\n",
    "df['Framerate'] = df['Framerate'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get distance to end\n",
    "df['Nose_to_end'] = 75 - (df['Nose_x']-df['Start_x_adjust'])/((df['End_x_adjust']-df['Start_x_adjust'])/75)\n",
    "df['TailBase_to_end'] = 75 - (df['TailBase_x']-df['Start_x_adjust'])/((df['End_x_adjust']-df['Start_x_adjust'])/75)\n",
    "df['Time_to_cross_s'] = df.groupby('Name File')['Framerate'].transform('size') / df['Framerate']\n",
    "\n",
    "# Create 'Failed_to_end' column when time to cross is too long\n",
    "df['Failed_to_end'] = df['Time_to_cross_s'] > 178\n",
    "\n",
    "def calculate_weighted_position(row):\n",
    "    if row['TailBase_to_end'] >= 75:\n",
    "        return row['Start_y_adjust']\n",
    "    elif row['TailBase_to_end'] <= 0:\n",
    "        return row['End_y_adjust']\n",
    "    else:\n",
    "        weight_start = (row['TailBase_to_end'] / 75)\n",
    "        weight_end = 1 - weight_start\n",
    "        return (weight_start * row['Start_y_adjust']) + (weight_end * row['End_y_adjust'])\n",
    "    \n",
    "# Apply the function to create the 'Position_y' column\n",
    "df['Position_y'] = df.apply(calculate_weighted_position, axis=1)\n",
    "\n",
    "#Get distance traveled\n",
    "df['Nose_diff'] = df['Nose_to_end'].diff().abs()\n",
    "df['TailBase_diff'] = df['TailBase_to_end'].diff().abs()\n",
    "df['Distance_traveled'] = (df['Nose_diff'] + df['TailBase_diff']) / 2\n",
    "df['Absolute_distance_traveled'] = df.groupby('Name File')['Distance_traveled'].transform('sum')\n",
    "df.drop(['Nose_diff', 'TailBase_diff'], axis=1, inplace=True)\n",
    "\n",
    "#Determine speed\n",
    "df['Speed_cm/s'] = df['Distance_traveled']*df['Framerate']\n",
    "\n",
    "# Calculate the condition for 'Start_box_nose' and set to False when the condition is not met\n",
    "#threshold = df['Start_x_adjust'] + (10 * ((df['End_x_adjust'] - df['Start_x_adjust']) / 75))\n",
    "#df['Start_box_nose'] = (df['Nose_x'] - df['Start_x_adjust']) < threshold\n",
    "#df['Start_box_nose'] = df['Start_box_nose'].fillna(False)\n",
    "\n",
    "# Calculate the condition for 'Start_box_tailbase' and set to False when the condition is not met\n",
    "#df['Start_box_tailbase'] = (df['TailBase_x'] - df['Start_x_adjust']) < threshold\n",
    "#df['Start_box_tailbase'] = df['Start_box_tailbase'].fillna(False)\n",
    "\n",
    "#Pawslips\n",
    "df['FrontLimb_slip'] = (df['FrontLimb_y'] > df['Position_y']) & (df['TailBase_x'] < (df['End_x_adjust'] - 1))\n",
    "df['FrontLimb_slip'] = df['FrontLimb_slip'].fillna(False)\n",
    "df['HindLimb_slip'] = (df['HindLimb_y'] > df['Position_y']) & (df['TailBase_x'] < (df['End_x_adjust'] - 1))\n",
    "df['HindLimb_slip'] = df['HindLimb_slip'].fillna(False) \n",
    "\n",
    "# Apply the function to create the 'Immobile' column\n",
    "df['Immobile'] = df['Speed_cm/s'] < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11edfd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom function to calculate 'Time_to_descend'\n",
    "def calculate_time_to_descend(group):\n",
    "    threshold = 7\n",
    "    if (group['TailBase_to_end'] <= threshold).any():\n",
    "        # Find the index of the first observation where 'TailBase_to_end' is below the threshold\n",
    "        index_below_threshold = (group['TailBase_to_end'] <= threshold).idxmax()\n",
    "        # Count the number of rows before the first observation below the threshold\n",
    "        rows_before_descend = index_below_threshold + 1\n",
    "        # Calculate 'Time_to_descend' by dividing rows_before_descend by the frame rate\n",
    "        return rows_before_descend / group['Framerate'].iloc[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Calculate 'Time_to_descend' for each 'Name File' group\n",
    "result = df.groupby('Name File').apply(lambda x: calculate_time_to_descend(x))\n",
    "\n",
    "# Merge the result back into the original DataFrame using 'Name File' as the key\n",
    "df = df.merge(result.rename('Time_to_descend'), left_on='Name File', right_index=True)\n",
    "\n",
    "#Get the time the mice spend in the end of the bar\n",
    "df['Time_to_descend_end'] = df['Time_to_cross_s'] - df['Time_to_descend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate time after a 7cm decrease\n",
    "def calculate_time_after_7cm(group):\n",
    "    initial_value = group['TailBase_to_end'].iat[0]  # Get the initial value for the group\n",
    "    threshold = 7\n",
    "\n",
    "    # Find the index of the first row where 'TailBase_to_end' decreases by 7 or more\n",
    "    first_decrease_idx = (group['TailBase_to_end'] <= (initial_value - threshold)).idxmax()\n",
    "\n",
    "    if not first_decrease_idx:\n",
    "        return np.nan  # If no such decrease is found, return NaN\n",
    "\n",
    "    # Calculate the time in seconds based on the index\n",
    "    time_seconds = first_decrease_idx / group['Framerate'].iat[0]\n",
    "    \n",
    "    return time_seconds\n",
    "\n",
    "# Apply the custom function to each 'Name File' group\n",
    "time_after_7cm = df.groupby('Name File').apply(calculate_time_after_7cm).reset_index()\n",
    "time_after_7cm.columns = ['Name File', 'Time_after_first_7cm']\n",
    "\n",
    "# Merge the result back to the original DataFrame\n",
    "df = df.merge(time_after_7cm, on='Name File', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r'C:\\Users\\UCPH\\gabi\\thesis analysis\\BB_preprocess.csv'  # Provide your desired file name\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(save_path, index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467f247",
   "metadata": {},
   "source": [
    "#### Don't run this any further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e719a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time_to_descend'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ee90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where 'Dragging' is True and select specified columns\n",
    "filtered_df = df[df['Failed_to_end']].copy()\n",
    "columns_of_interest = ['Name File','Failed_to_end']\n",
    "filtered_df = filtered_df[columns_of_interest]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "filtered_df['Name File'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93072c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where 'Filed_to_end' is True and select specified columns\n",
    "filtered_df = df[df['Failed_to_end']].copy()\n",
    "columns_of_interest = ['Name File', 'Failed_to_end']\n",
    "filtered_df = filtered_df[columns_of_interest]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "filtered_df['Name File'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02ef965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time_to_cross_s'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d6b2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'y_pixels' and 'Framerate' to integers\n",
    "df['y_pixels'] = df['y_pixels'].astype(int)\n",
    "df['Framerate'] = df['Framerate'].astype(int)\n",
    "df['TailBase_y'] = df['TailBase_y'].astype(float)\n",
    "df['Position_y'] = df['Position_y'].astype(float)\n",
    "\n",
    "\n",
    "# Define a function to check the Dragging criteria\n",
    "def check_dragging(row):\n",
    "    if row['y_pixels'] == 720:\n",
    "        threshold = 10\n",
    "    elif row['y_pixels'] == 1080:\n",
    "        threshold = (720 / 1080) * 10\n",
    "    else:\n",
    "        return None  # Return None if 'y_pixels' is not 720 or 1080\n",
    "\n",
    "    # Create a dictionary to keep track of unique name files and their counts\n",
    "    unique_name_files = {}\n",
    "    \n",
    "    if row['Framerate'] == 15:\n",
    "        required_rows = 3\n",
    "    elif row['Framerate'] == 60:\n",
    "        required_rows = 12\n",
    "    else:\n",
    "        return None  # Return None if 'Framerate' is neither 15 nor 60\n",
    "\n",
    "    consecutive_count = 0  # Initialize a count for consecutive rows\n",
    "\n",
    "    for _, group_row in df.iterrows():\n",
    "        if (\n",
    "            abs(row['Position_y'] - group_row['TailBase_y']) <= threshold\n",
    "            and not group_row['Immobile']\n",
    "        ):\n",
    "            name_file = group_row['Name File']\n",
    "            \n",
    "            if name_file not in unique_name_files:\n",
    "                unique_name_files[name_file] = 1  # First occurrence of the name file\n",
    "            else:\n",
    "                unique_name_files[name_file] += 1\n",
    "                \n",
    "                if unique_name_files[name_file] > 1 and not row['Immobile']:\n",
    "                    consecutive_count += 1\n",
    "                    if consecutive_count >= required_rows:\n",
    "                        return True\n",
    "        else:\n",
    "            consecutive_count = 0  # Reset the count if the criteria aren't met\n",
    "\n",
    "    return False  # Return False if consecutive required frames are not found\n",
    "\n",
    "# Apply the function to create the 'Dragging' column\n",
    "df['Dragging'] = df.apply(check_dragging, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine Framerate and y_pixels based on Cohort\n",
    "def set_threshold(row):\n",
    "    if row['y_pixels'] == 720:\n",
    "         return 10\n",
    "    elif row['y_pixels'] == 1080:\n",
    "        return (720 / 1080) * 10\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the 'Framerate' and 'y_pixels' columns\n",
    "df['Threshold'] = df.apply(set_threshold, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deb0a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where 'Dragging' is True and select specified columns\n",
    "filtered_df = df[df['Dragging']].copy()\n",
    "columns_of_interest = ['Framerate', 'y_pixels', 'Name File', 'Position_y', 'TailBase_y', 'Threshold','Immobile', 'Dragging']\n",
    "filtered_df = filtered_df[columns_of_interest]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['TailBase_to_end', 'Start_y_adjust','End_y_adjust', 'Position_y']].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d966474",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1134aac8",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d89bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes the input file and returns a dataframe with the angles (both corrected and uncorrected)\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    df : pandas dataframe\n",
    "        output .csv from the deeplabcut analysis.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "        Same dataframe as input cleaner with generlized column names.\n",
    "    \"\"\"\n",
    "\n",
    "    #seperate x, y and likelihood into 3 dfs for renaming and preprocessing\n",
    "    df_x = df[df.columns[[1, 4, 7, 10, 13, 16]]]\n",
    "    df_y = df[df.columns[[2, 5, 8, 11, 14, 17]]]\n",
    "    df_like = df[df.columns[[3, 6, 9, 12, 15, 18]]]\n",
    "\n",
    "    #drop the unneeded cols\n",
    "    df_x = df_x.drop(0)\n",
    "    df_y = df_y.drop(0)\n",
    "    df_x = df_x.drop(2)\n",
    "    df_y = df_y.drop(2)\n",
    "\n",
    "    df_like = df_like.drop(0)\n",
    "    df_like = df_like.drop(2)\n",
    "\n",
    "    #Rename variables\n",
    "    df_x.columns=df_x.iloc[0]\n",
    "    df_x = df_x.drop(1)\n",
    "    df_x.rename(columns = {'Nose':'Nose_x', 'Frontlimb':'FrontLimb_x', 'HindLimb':'HindLimb_x', 'TailBase':'TailBase_x', 'Start':'Start_x', 'End':'End_x'}, inplace = True)\n",
    "\n",
    "\n",
    "    df_y.columns=df_y.iloc[0]\n",
    "    df_y = df_y.drop(1)\n",
    "    df_y.rename(columns = {'Nose':'Nose_y', 'Frontlimb':'FrontLimb_y', 'HindLimb':'HindLimb_y', 'TailBase':'TailBase_y', 'Start':'Start_y', 'End':'End_y'}, inplace = True)\n",
    "\n",
    "\n",
    "    df_like.columns=df_like.iloc[0]\n",
    "    df_like = df_like.drop(1)\n",
    "    df_like.rename(columns = {'Nose':'Nose_likelihood', 'Frontlimb':'FrontLimb_likelihood', 'HindLimb':'HindLimb_likelihood', 'TailBase':'TailBase_likelihood', 'Start':'Start_likelihood', 'End':'End_likelihood'}, inplace = True)\n",
    "\n",
    "    #merge the dataframes\n",
    "    df_all = df_x.join(df_y)\n",
    "    df_all = df_all.join(df_like)\n",
    "    df_all = df_all.astype(float)\n",
    "\n",
    "    return df_all\n",
    "\n",
    " \n",
    "\n",
    "def files_from_folder(path):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes the input path and returns a dataframe with the concatanated data found in the folder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Path : input path for the folder where the files to be loaded are\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    df : pandas dataframe\n",
    "        A concatanated dataframe with all the values\n",
    "    \"\"\"\n",
    "\n",
    "    import glob\n",
    "    import os.path\n",
    "    import pandas as pd\n",
    "\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    df = []\n",
    "\n",
    "    # loop over the list of csv files\n",
    "    for f in csv_files:\n",
    "\n",
    "        # read the csv file\n",
    "        file = pd.read_csv(f, header = None)\n",
    "        file = preprocess(file)\n",
    "        file['Name File'] = os.path.basename(f)\n",
    "        file['Name File'] = file['Name File'].str.split('DLC_').str[0]\n",
    "        df.append(file)\n",
    "    df_con = pd.concat(df)\n",
    "\n",
    "    return df_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509eaac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
