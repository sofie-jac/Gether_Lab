{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c77fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "#from ipynb.fs.full.BB_functions import files_from_folder, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0eab208",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = Path(r'C:\\Users\\UCPH\\gabi\\thesis analysis\\test1')\n",
    "input_path2 = Path(r'C:\\Users\\UCPH\\gabi\\thesis analysis\\test2')\n",
    "save_path = Path(r'C:\\Users\\UCPH\\gabi\\thesis analysis')\n",
    "\n",
    "combined_path = os.path.join(input_path, input_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6d7780",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = files_from_folder(combined_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f98cf3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>1</th>\n",
       "      <th>Nose_x</th>\n",
       "      <th>FrontLimb_x</th>\n",
       "      <th>HindLimb_x</th>\n",
       "      <th>TailBase_x</th>\n",
       "      <th>Start_x</th>\n",
       "      <th>End_x</th>\n",
       "      <th>Nose_y</th>\n",
       "      <th>FrontLimb_y</th>\n",
       "      <th>HindLimb_y</th>\n",
       "      <th>TailBase_y</th>\n",
       "      <th>Start_y</th>\n",
       "      <th>End_y</th>\n",
       "      <th>Nose_likelihood</th>\n",
       "      <th>FrontLimb_likelihood</th>\n",
       "      <th>HindLimb_likelihood</th>\n",
       "      <th>TailBase_likelihood</th>\n",
       "      <th>Start_likelihood</th>\n",
       "      <th>End_likelihood</th>\n",
       "      <th>Name File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>181.575607</td>\n",
       "      <td>142.569519</td>\n",
       "      <td>62.056583</td>\n",
       "      <td>55.628613</td>\n",
       "      <td>32.686756</td>\n",
       "      <td>1112.671997</td>\n",
       "      <td>227.590363</td>\n",
       "      <td>233.456985</td>\n",
       "      <td>226.868454</td>\n",
       "      <td>191.981308</td>\n",
       "      <td>233.465683</td>\n",
       "      <td>220.942612</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.992205</td>\n",
       "      <td>0.999664</td>\n",
       "      <td>0.999892</td>\n",
       "      <td>0.996044</td>\n",
       "      <td>0.999849</td>\n",
       "      <td>1_POST_BB_15_H0,2_KI_M_T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>202.752014</td>\n",
       "      <td>145.496994</td>\n",
       "      <td>72.242058</td>\n",
       "      <td>72.860168</td>\n",
       "      <td>36.782040</td>\n",
       "      <td>1112.708740</td>\n",
       "      <td>225.885925</td>\n",
       "      <td>234.878067</td>\n",
       "      <td>221.357727</td>\n",
       "      <td>205.209290</td>\n",
       "      <td>230.039444</td>\n",
       "      <td>220.584717</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999848</td>\n",
       "      <td>0.989090</td>\n",
       "      <td>0.999715</td>\n",
       "      <td>0.999868</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>1_POST_BB_15_H0,2_KI_M_T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>202.752655</td>\n",
       "      <td>145.553162</td>\n",
       "      <td>72.486900</td>\n",
       "      <td>72.640656</td>\n",
       "      <td>35.920845</td>\n",
       "      <td>1112.722290</td>\n",
       "      <td>225.888199</td>\n",
       "      <td>234.864944</td>\n",
       "      <td>221.085968</td>\n",
       "      <td>205.038559</td>\n",
       "      <td>229.206589</td>\n",
       "      <td>220.572021</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999859</td>\n",
       "      <td>0.989455</td>\n",
       "      <td>0.999701</td>\n",
       "      <td>0.999917</td>\n",
       "      <td>0.999866</td>\n",
       "      <td>1_POST_BB_15_H0,2_KI_M_T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>226.492004</td>\n",
       "      <td>189.416916</td>\n",
       "      <td>116.875633</td>\n",
       "      <td>91.615646</td>\n",
       "      <td>36.225803</td>\n",
       "      <td>1113.403198</td>\n",
       "      <td>225.756989</td>\n",
       "      <td>230.018616</td>\n",
       "      <td>236.289902</td>\n",
       "      <td>210.197876</td>\n",
       "      <td>230.437134</td>\n",
       "      <td>221.220551</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999641</td>\n",
       "      <td>0.999813</td>\n",
       "      <td>0.999920</td>\n",
       "      <td>0.999904</td>\n",
       "      <td>0.999834</td>\n",
       "      <td>1_POST_BB_15_H0,2_KI_M_T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>244.978149</td>\n",
       "      <td>195.232849</td>\n",
       "      <td>117.810814</td>\n",
       "      <td>111.228165</td>\n",
       "      <td>34.076157</td>\n",
       "      <td>1114.256470</td>\n",
       "      <td>224.095917</td>\n",
       "      <td>228.623718</td>\n",
       "      <td>233.531265</td>\n",
       "      <td>210.584732</td>\n",
       "      <td>229.465210</td>\n",
       "      <td>221.240082</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999948</td>\n",
       "      <td>0.999902</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>0.997012</td>\n",
       "      <td>0.999675</td>\n",
       "      <td>1_POST_BB_15_H0,2_KI_M_T1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1203.319580</td>\n",
       "      <td>1167.914917</td>\n",
       "      <td>1144.543945</td>\n",
       "      <td>1111.087524</td>\n",
       "      <td>25.176291</td>\n",
       "      <td>1108.187500</td>\n",
       "      <td>238.343262</td>\n",
       "      <td>234.402145</td>\n",
       "      <td>234.387344</td>\n",
       "      <td>221.850952</td>\n",
       "      <td>227.374435</td>\n",
       "      <td>217.696838</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999796</td>\n",
       "      <td>0.989164</td>\n",
       "      <td>0.998271</td>\n",
       "      <td>0.999931</td>\n",
       "      <td>0.598016</td>\n",
       "      <td>1_PRE_BB_37_H0,8_WT_M_T4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1203.306641</td>\n",
       "      <td>1167.927124</td>\n",
       "      <td>1144.628784</td>\n",
       "      <td>1111.127197</td>\n",
       "      <td>25.314110</td>\n",
       "      <td>1108.174927</td>\n",
       "      <td>238.314209</td>\n",
       "      <td>234.348450</td>\n",
       "      <td>234.411850</td>\n",
       "      <td>221.887390</td>\n",
       "      <td>227.199097</td>\n",
       "      <td>217.902237</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>0.989757</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>0.559191</td>\n",
       "      <td>1_PRE_BB_37_H0,8_WT_M_T4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1202.397705</td>\n",
       "      <td>1167.772827</td>\n",
       "      <td>1147.985474</td>\n",
       "      <td>1116.746582</td>\n",
       "      <td>25.202160</td>\n",
       "      <td>1109.177856</td>\n",
       "      <td>234.186935</td>\n",
       "      <td>236.497589</td>\n",
       "      <td>239.568069</td>\n",
       "      <td>237.369385</td>\n",
       "      <td>227.330368</td>\n",
       "      <td>229.688782</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999871</td>\n",
       "      <td>0.949231</td>\n",
       "      <td>0.999340</td>\n",
       "      <td>0.999928</td>\n",
       "      <td>0.029175</td>\n",
       "      <td>1_PRE_BB_37_H0,8_WT_M_T4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1189.521851</td>\n",
       "      <td>1175.634277</td>\n",
       "      <td>1154.050903</td>\n",
       "      <td>1114.361084</td>\n",
       "      <td>25.198122</td>\n",
       "      <td>1115.891357</td>\n",
       "      <td>236.058151</td>\n",
       "      <td>235.648041</td>\n",
       "      <td>239.022568</td>\n",
       "      <td>231.595230</td>\n",
       "      <td>227.399902</td>\n",
       "      <td>231.952866</td>\n",
       "      <td>0.998705</td>\n",
       "      <td>0.999117</td>\n",
       "      <td>0.991880</td>\n",
       "      <td>0.999832</td>\n",
       "      <td>0.999925</td>\n",
       "      <td>0.041935</td>\n",
       "      <td>1_PRE_BB_37_H0,8_WT_M_T4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1189.513916</td>\n",
       "      <td>1175.691284</td>\n",
       "      <td>1154.058228</td>\n",
       "      <td>1114.332153</td>\n",
       "      <td>25.212086</td>\n",
       "      <td>1115.904663</td>\n",
       "      <td>236.044342</td>\n",
       "      <td>235.643097</td>\n",
       "      <td>239.057068</td>\n",
       "      <td>231.552185</td>\n",
       "      <td>227.485458</td>\n",
       "      <td>232.017044</td>\n",
       "      <td>0.998657</td>\n",
       "      <td>0.999108</td>\n",
       "      <td>0.991835</td>\n",
       "      <td>0.999836</td>\n",
       "      <td>0.999927</td>\n",
       "      <td>0.041210</td>\n",
       "      <td>1_PRE_BB_37_H0,8_WT_M_T4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69531 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "1        Nose_x  FrontLimb_x   HindLimb_x   TailBase_x    Start_x  \\\n",
       "3    181.575607   142.569519    62.056583    55.628613  32.686756   \n",
       "4    202.752014   145.496994    72.242058    72.860168  36.782040   \n",
       "5    202.752655   145.553162    72.486900    72.640656  35.920845   \n",
       "6    226.492004   189.416916   116.875633    91.615646  36.225803   \n",
       "7    244.978149   195.232849   117.810814   111.228165  34.076157   \n",
       "..          ...          ...          ...          ...        ...   \n",
       "86  1203.319580  1167.914917  1144.543945  1111.087524  25.176291   \n",
       "87  1203.306641  1167.927124  1144.628784  1111.127197  25.314110   \n",
       "88  1202.397705  1167.772827  1147.985474  1116.746582  25.202160   \n",
       "89  1189.521851  1175.634277  1154.050903  1114.361084  25.198122   \n",
       "90  1189.513916  1175.691284  1154.058228  1114.332153  25.212086   \n",
       "\n",
       "1         End_x      Nose_y  FrontLimb_y  HindLimb_y  TailBase_y     Start_y  \\\n",
       "3   1112.671997  227.590363   233.456985  226.868454  191.981308  233.465683   \n",
       "4   1112.708740  225.885925   234.878067  221.357727  205.209290  230.039444   \n",
       "5   1112.722290  225.888199   234.864944  221.085968  205.038559  229.206589   \n",
       "6   1113.403198  225.756989   230.018616  236.289902  210.197876  230.437134   \n",
       "7   1114.256470  224.095917   228.623718  233.531265  210.584732  229.465210   \n",
       "..          ...         ...          ...         ...         ...         ...   \n",
       "86  1108.187500  238.343262   234.402145  234.387344  221.850952  227.374435   \n",
       "87  1108.174927  238.314209   234.348450  234.411850  221.887390  227.199097   \n",
       "88  1109.177856  234.186935   236.497589  239.568069  237.369385  227.330368   \n",
       "89  1115.891357  236.058151   235.648041  239.022568  231.595230  227.399902   \n",
       "90  1115.904663  236.044342   235.643097  239.057068  231.552185  227.485458   \n",
       "\n",
       "1        End_y  Nose_likelihood  FrontLimb_likelihood  HindLimb_likelihood  \\\n",
       "3   220.942612         1.000000              0.992205             0.999664   \n",
       "4   220.584717         0.999996              0.999848             0.989090   \n",
       "5   220.572021         0.999996              0.999859             0.989455   \n",
       "6   221.220551         0.999999              0.999641             0.999813   \n",
       "7   221.240082         0.999999              0.999948             0.999902   \n",
       "..         ...              ...                   ...                  ...   \n",
       "86  217.696838         0.999997              0.999796             0.989164   \n",
       "87  217.902237         0.999997              0.999795             0.989757   \n",
       "88  229.688782         0.999998              0.999871             0.949231   \n",
       "89  231.952866         0.998705              0.999117             0.991880   \n",
       "90  232.017044         0.998657              0.999108             0.991835   \n",
       "\n",
       "1   TailBase_likelihood  Start_likelihood  End_likelihood  \\\n",
       "3              0.999892          0.996044        0.999849   \n",
       "4              0.999715          0.999868        0.999867   \n",
       "5              0.999701          0.999917        0.999866   \n",
       "6              0.999920          0.999904        0.999834   \n",
       "7              0.999949          0.997012        0.999675   \n",
       "..                  ...               ...             ...   \n",
       "86             0.998271          0.999931        0.598016   \n",
       "87             0.998333          0.999912        0.559191   \n",
       "88             0.999340          0.999928        0.029175   \n",
       "89             0.999832          0.999925        0.041935   \n",
       "90             0.999836          0.999927        0.041210   \n",
       "\n",
       "1                   Name File  \n",
       "3   1_POST_BB_15_H0,2_KI_M_T1  \n",
       "4   1_POST_BB_15_H0,2_KI_M_T1  \n",
       "5   1_POST_BB_15_H0,2_KI_M_T1  \n",
       "6   1_POST_BB_15_H0,2_KI_M_T1  \n",
       "7   1_POST_BB_15_H0,2_KI_M_T1  \n",
       "..                        ...  \n",
       "86   1_PRE_BB_37_H0,8_WT_M_T4  \n",
       "87   1_PRE_BB_37_H0,8_WT_M_T4  \n",
       "88   1_PRE_BB_37_H0,8_WT_M_T4  \n",
       "89   1_PRE_BB_37_H0,8_WT_M_T4  \n",
       "90   1_PRE_BB_37_H0,8_WT_M_T4  \n",
       "\n",
       "[69531 rows x 19 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'Name File' column into multiple columns\n",
    "df[['Cohort', 'Pre-Post', 'Test_Type', 'Animal_ID', 'Drug_Dose', 'Genotype', 'Gender', 'Trial']] = df['Name File'].str.split('_', expand=True)\n",
    "\n",
    "# Drop the original 'Name File' column if needed\n",
    "# df.drop('Name File', axis=1, inplace=True)\n",
    "\n",
    "# Split 'drug_dose' into 'Drug' and 'Dose' columns\n",
    "df[['Drug', 'Dose']] = df['Drug_Dose'].str.extract(r'([A-Za-z]+)(\\d+,\\d+)')\n",
    "\n",
    "# Extract only numbers from the 'Trial' column\n",
    "df['Trial'] = df['Trial'].str.extract('(\\d+)')\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12352d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through columns and set values to NaN based on condition\n",
    "for col in df.columns:\n",
    "    if col.endswith('_likelihood'):\n",
    "        mask = df[col] < 0.7\n",
    "        prefix = col[:-11]  # Remove '_likelihood' to get the prefix\n",
    "        for suffix in ['x', 'y']:\n",
    "            column_to_set_nan = f'{prefix}_{suffix}'\n",
    "            df.loc[mask, column_to_set_nan] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5811945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the adjusted mean\n",
    "def calculate_adjusted_mean(group):\n",
    "    filtered_start_x = group[group['Start_likelihood'] > 0.95]['Start_x']\n",
    "    filtered_start_y = group[group['Start_likelihood'] > 0.95]['Start_y']\n",
    "    filtered_end_x = group[group['End_likelihood'] > 0.95]['End_x']\n",
    "    filtered_end_y = group[group['End_likelihood'] > 0.95]['End_y']\n",
    "    \n",
    "    start_x_adjusted_mean = filtered_start_x.mean()\n",
    "    start_y_adjusted_mean = filtered_start_y.mean()\n",
    "    end_x_adjusted_mean = filtered_end_x.mean()\n",
    "    end_y_adjusted_mean = filtered_end_y.mean()\n",
    "    \n",
    "    group['Start_x_adjust'] = start_x_adjusted_mean\n",
    "    group['Start_y_adjust'] = start_y_adjusted_mean\n",
    "    group['End_x_adjust'] = end_x_adjusted_mean\n",
    "    group['End_y_adjust'] = end_y_adjusted_mean\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Group by 'Name File' and apply the calculation\n",
    "df = df.groupby('Name File').apply(calculate_adjusted_mean)\n",
    "\n",
    "# Fill NaN values in the new columns with appropriate values\n",
    "df['Start_x_adjust'].fillna(df['Start_x'], inplace=True)\n",
    "df['Start_y_adjust'].fillna(df['Start_y'], inplace=True)\n",
    "df['End_x_adjust'].fillna(df['End_x'], inplace=True)\n",
    "df['End_y_adjust'].fillna(df['End_y'], inplace=True)\n",
    "\n",
    "# Drop the original 'Start_likelihood' and 'End_likelihood' columns if needed\n",
    "# df.drop(['Start_likelihood', 'End_likelihood'], axis=1, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1141e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine Framerate and y_pixels based on Cohort\n",
    "def set_framerate_and_ypixels(row):\n",
    "    if row['Cohort'] == 1:\n",
    "        return 15, 720\n",
    "    elif row['Cohort'] == 2:\n",
    "        return 60, 1080\n",
    "    else:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "# Convert 'Cohort' to integer\n",
    "df['Cohort'] = df['Cohort'].astype(int)\n",
    "\n",
    "# Apply the function to create the 'Framerate' and 'y_pixels' columns\n",
    "df[['Framerate', 'y_pixels']] = df.apply(set_framerate_and_ypixels, axis=1, result_type='expand')\n",
    "df['Framerate'] = df['Framerate'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get distance to end\n",
    "df['Nose_to_end'] = 75 - (df['Nose_x']-df['Start_x_adjust'])/((df['End_x_adjust']-df['Start_x_adjust'])/75)\n",
    "df['TailBase_to_end'] = 75 - (df['TailBase_x']-df['Start_x_adjust'])/((df['End_x_adjust']-df['Start_x_adjust'])/75)\n",
    "df['Time_to_cross_s'] = df.groupby('Name File')['Framerate'].transform('size') / df['Framerate']\n",
    "\n",
    "# Create 'Failed_to_end' column with True/False values based on if it's more than 5 cm away from end on the last frame of each trial\n",
    "df['Failed_to_end'] = df.groupby('Name File')['TailBase_to_end'].transform(lambda x: x.iloc[-1] > 1 )\n",
    "\n",
    "def calculate_weighted_position(row):\n",
    "    if row['TailBase_to_end'] >= 75:\n",
    "        return row['Start_y_adjust']\n",
    "    elif row['TailBase_to_end'] <= 0:\n",
    "        return row['End_y_adjust']\n",
    "    else:\n",
    "        weight_start = (row['TailBase_to_end'] / 75)\n",
    "        weight_end = 1 - weight_start\n",
    "        return (weight_start * row['Start_y_adjust']) + (weight_end * row['End_y_adjust'])\n",
    "    \n",
    "# Apply the function to create the 'Position_y' column\n",
    "df['Position_y'] = df.apply(calculate_weighted_position, axis=1)\n",
    "\n",
    "# Apply the function to create the 'Position_y' column\n",
    "df['Position_y'] = df.apply(calculate_weighted_position, axis=1)\n",
    "#Get distance traveled\n",
    "df['Nose_diff'] = df['Nose_to_end'].diff().abs()\n",
    "df['TailBase_diff'] = df['TailBase_to_end'].diff().abs()\n",
    "df['Distance_traveled'] = (df['Nose_diff'] + df['TailBase_diff']) / 2\n",
    "df['Absolute_distance_traveled'] = df.groupby('Name File')['Distance_traveled'].transform('sum')\n",
    "\n",
    "#Determine speed\n",
    "df['Speed_cm/s'] = df['Distance_traveled']*df['Framerate']\n",
    "\n",
    "# Calculate the condition for 'Start_box_nose' and set to False when the condition is not met\n",
    "threshold = df['Start_x_adjust'] + (10 * ((df['End_x_adjust'] - df['Start_x_adjust']) / 75))\n",
    "df['Start_box_nose'] = (df['Nose_x'] - df['Start_x_adjust']) < threshold\n",
    "df['Start_box_nose'] = df['Start_box_nose'].fillna(False)\n",
    "\n",
    "# Calculate the condition for 'Start_box_tailbase' and set to False when the condition is not met\n",
    "df['Start_box_tailbase'] = (df['TailBase_x'] - df['Start_x_adjust']) < threshold\n",
    "df['Start_box_tailbase'] = df['Start_box_tailbase'].fillna(False)\n",
    "\n",
    "#Pawslips\n",
    "df['FrontLimb_slip'] = (df['FrontLimb_y'] > df['Position_y']) & (df['TailBase_x'] < (df['End_x_adjust'] - 1))\n",
    "df['FrontLimb_slip'] = df['FrontLimb_slip'].fillna(False)\n",
    "df['HindLimb_slip'] = (df['HindLimb_y'] > df['Position_y']) & (df['TailBase_x'] < (df['End_x_adjust'] - 1))\n",
    "df['HindLimb_slip'] = df['HindLimb_slip'].fillna(False) \n",
    "\n",
    "# Apply the function to create the 'Immobile' column\n",
    "df['Immobile'] = df['Speed_cm/s'] < 0.05 * df['Speed_cm/s'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ab4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom function to calculate 'Time_to_descend'\n",
    "def calculate_time_to_descend(group):\n",
    "    threshold = 7\n",
    "    if (group['TailBase_to_end'] <= threshold).any():\n",
    "        # Find the index of the first observation where 'TailBase_to_end' is below the threshold\n",
    "        index_below_threshold = (group['TailBase_to_end'] <= threshold).idxmax()\n",
    "        # Count the number of rows before the first observation below the threshold\n",
    "        rows_before_descend = index_below_threshold + 1\n",
    "        # Calculate 'Time_to_descend' by dividing rows_before_descend by the frame rate\n",
    "        return rows_before_descend / group['Framerate'].iloc[0]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Calculate 'Time_to_descend' for each 'Name File' group\n",
    "result = df.groupby('Name File').apply(lambda x: calculate_time_to_descend(x))\n",
    "\n",
    "# Merge the result back into the original DataFrame using 'Name File' as the key\n",
    "df = df.merge(result.rename('Time_to_descend'), left_on='Name File', right_index=True)\n",
    "\n",
    "#Get the time the mice spend in the end of the bar\n",
    "df['Time_to_descend_end'] = df['Time_to_cross_s'] - df['Time_to_descend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a218cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate time after a 7cm decrease\n",
    "def calculate_time_after_7cm(group):\n",
    "    initial_value = group['TailBase_to_end'].iat[0]  # Get the initial value for the group\n",
    "    threshold = 7\n",
    "\n",
    "    # Find the index of the first row where 'TailBase_to_end' decreases by 7 or more\n",
    "    first_decrease_idx = (group['TailBase_to_end'] <= (initial_value - threshold)).idxmax()\n",
    "\n",
    "    if not first_decrease_idx:\n",
    "        return np.nan  # If no such decrease is found, return NaN\n",
    "\n",
    "    # Calculate the time in seconds based on the index\n",
    "    time_seconds = first_decrease_idx / group['Framerate'].iat[0]\n",
    "    \n",
    "    return time_seconds\n",
    "\n",
    "# Apply the custom function to each 'Name File' group\n",
    "time_after_7cm = df.groupby('Name File').apply(calculate_time_after_7cm).reset_index()\n",
    "time_after_7cm.columns = ['Name File', 'Time_after_first_7cm']\n",
    "\n",
    "# Merge the result back to the original DataFrame\n",
    "df = df.merge(time_after_7cm, on='Name File', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c4909",
   "metadata": {},
   "source": [
    "##### Save the data frame here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7810268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r'C:\\Users\\UCPH\\gabi\\thesis analysis\\BB_preprocess.csv'  # Provide your desired file name\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(save_path, index=False)  # Set index=False to exclude the index column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99627da9",
   "metadata": {},
   "source": [
    "#### Don't run this any further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4aa214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where 'Dragging' is True and select specified columns\n",
    "filtered_df = df[df['Failed_to_end']].copy()\n",
    "columns_of_interest = ['Name File', 'TailBase_to_end','Failed_to_end', 'Time_to_descend', 'Time_to_cross_s']\n",
    "filtered_df = filtered_df[columns_of_interest]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "filtered_df['Name File'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ce70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['Time_to_cross_s'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ad841",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a dragging collumn\n",
    "\n",
    "# Ensure 'y_pixels' and 'Framerate' are integers\n",
    "#df['y_pixels'] = df['y_pixels'].astype(int)\n",
    "#df['Framerate'] = df['Framerate'].astype(int)\n",
    "\n",
    "# Calculate the threshold for 'TailBase_y' based on 'y_pixels'\n",
    "#df['threshold_y'] = np.where(df['y_pixels'] == 720, 10, (720 / 1080) * 10)\n",
    "\n",
    "# Define a function to check the dragging condition\n",
    "#def check_dragging_condition(group):\n",
    " #   return group['TailBase_y'].gt(group['Position_y'] + group['threshold_y']).all() and group['Immobile'].all()\n",
    "\n",
    "# Apply the function to create the 'Dragging' column\n",
    "#df['Dragging'] = False\n",
    "\n",
    "# Check the dragging condition for each group of rows\n",
    "#for _, group in df.groupby('Name File'):\n",
    " #   if group['Framerate'].iloc[0] == 15:\n",
    "  #      if group.shape[0] >= 5 and not check_dragging_condition(group):\n",
    "   #         df.loc[group.index, 'Dragging'] = True\n",
    "    #elif group['Framerate'].iloc[0] == 60:\n",
    "     #   if group.shape[0] >= 20 and not check_dragging_condition(group):\n",
    "      #      df.loc[group.index, 'Dragging'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6125dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'y_pixels' and 'Framerate' to integers\n",
    "df['y_pixels'] = df['y_pixels'].astype(int)\n",
    "df['Framerate'] = df['Framerate'].astype(int)\n",
    "df['TailBase_y'] = df['TailBase_y'].astype(float)\n",
    "df['Position_y'] = df['Position_y'].astype(float)\n",
    "\n",
    "# Define a function to check the Dragging criteria\n",
    "def check_dragging(row):\n",
    "    if row['y_pixels'] == 720:\n",
    "        threshold = 10\n",
    "    elif row['y_pixels'] == 1080:\n",
    "        threshold = (720 / 1080) * 10\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    name_file_group = df.groupby('Name File').get_group(row['Name File'])\n",
    "    if row['Framerate'] == 15:\n",
    "        required_rows = 3\n",
    "    elif row['Framerate'] == 60:\n",
    "        required_rows = 12\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    consecutive_count = 0  # Initialize a count for consecutive rows\n",
    "\n",
    "    for _, group_row in name_file_group.iterrows():\n",
    "        if (\n",
    "            group_row['TailBase_y'] > (row['Position_y'] - threshold)\n",
    "            and not group_row['Immobile']\n",
    "        ):\n",
    "            consecutive_count += 1\n",
    "            if consecutive_count >= required_rows:\n",
    "                return True\n",
    "        else:\n",
    "            consecutive_count = 0  # Reset the count if the criteria aren't met\n",
    "\n",
    "    return False  # Return False if consecutive required frames are not found\n",
    "\n",
    "# Apply the function to create the 'Dragging' column\n",
    "df['Dragging'] = df.apply(check_dragging, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bfbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to determine Framerate and y_pixels based on Cohort\n",
    "def set_threshold(row):\n",
    "    if row['y_pixels'] == 720:\n",
    "         return 10\n",
    "    elif row['y_pixels'] == 1080:\n",
    "        return (720 / 1080) * 10\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the function to create the 'Framerate' and 'y_pixels' columns\n",
    "df['Threshold'] = df.apply(set_threshold, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdfeed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where 'Dragging' is True and select specified columns\n",
    "filtered_df = df[df['Dragging']].copy()\n",
    "columns_of_interest = ['Framerate', 'y_pixels', 'Name File', 'Position_y', 'TailBase_y', 'Threshold','Immobile', 'Dragging']\n",
    "filtered_df = filtered_df[columns_of_interest]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['TailBase_to_end', 'Start_y_adjust','End_y_adjust', 'Position_y']].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1134aac8",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d89bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes the input file and returns a dataframe with the angles (both corrected and uncorrected)\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    df : pandas dataframe\n",
    "        output .csv from the deeplabcut analysis.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "        Same dataframe as input cleaner with generlized column names.\n",
    "    \"\"\"\n",
    "\n",
    "    #seperate x, y and likelihood into 3 dfs for renaming and preprocessing\n",
    "    df_x = df[df.columns[[1, 4, 7, 10, 13, 16]]]\n",
    "    df_y = df[df.columns[[2, 5, 8, 11, 14, 17]]]\n",
    "    df_like = df[df.columns[[3, 6, 9, 12, 15, 18]]]\n",
    "\n",
    "    #drop the unneeded cols\n",
    "    df_x = df_x.drop(0)\n",
    "    df_y = df_y.drop(0)\n",
    "    df_x = df_x.drop(2)\n",
    "    df_y = df_y.drop(2)\n",
    "\n",
    "    df_like = df_like.drop(0)\n",
    "    df_like = df_like.drop(2)\n",
    "\n",
    "    #Rename variables\n",
    "    df_x.columns=df_x.iloc[0]\n",
    "    df_x = df_x.drop(1)\n",
    "    df_x.rename(columns = {'Nose':'Nose_x', 'Frontlimb':'FrontLimb_x', 'HindLimb':'HindLimb_x', 'TailBase':'TailBase_x', 'Start':'Start_x', 'End':'End_x'}, inplace = True)\n",
    "\n",
    "\n",
    "    df_y.columns=df_y.iloc[0]\n",
    "    df_y = df_y.drop(1)\n",
    "    df_y.rename(columns = {'Nose':'Nose_y', 'Frontlimb':'FrontLimb_y', 'HindLimb':'HindLimb_y', 'TailBase':'TailBase_y', 'Start':'Start_y', 'End':'End_y'}, inplace = True)\n",
    "\n",
    "\n",
    "    df_like.columns=df_like.iloc[0]\n",
    "    df_like = df_like.drop(1)\n",
    "    df_like.rename(columns = {'Nose':'Nose_likelihood', 'Frontlimb':'FrontLimb_likelihood', 'HindLimb':'HindLimb_likelihood', 'TailBase':'TailBase_likelihood', 'Start':'Start_likelihood', 'End':'End_likelihood'}, inplace = True)\n",
    "\n",
    "    #merge the dataframes\n",
    "    df_all = df_x.join(df_y)\n",
    "    df_all = df_all.join(df_like)\n",
    "    df_all = df_all.astype(float)\n",
    "\n",
    "    return df_all\n",
    "\n",
    " \n",
    "\n",
    "def files_from_folder(path):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes the input path and returns a dataframe with the concatanated data found in the folder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Path : input path for the folder where the files to be loaded are\n",
    "\n",
    "    Returns\n",
    "    ------\n",
    "    df : pandas dataframe\n",
    "        A concatanated dataframe with all the values\n",
    "    \"\"\"\n",
    "\n",
    "    import glob\n",
    "    import os.path\n",
    "    import pandas as pd\n",
    "\n",
    "    csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    df = []\n",
    "\n",
    "    # loop over the list of csv files\n",
    "    for f in csv_files:\n",
    "\n",
    "        # read the csv file\n",
    "        file = pd.read_csv(f, header = None)\n",
    "        file = preprocess(file)\n",
    "        file['Name File'] = os.path.basename(f)\n",
    "        file['Name File'] = file['Name File'].str.split('DLC_').str[0]\n",
    "        df.append(file)\n",
    "    df_con = pd.concat(df)\n",
    "\n",
    "    return df_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509eaac9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
